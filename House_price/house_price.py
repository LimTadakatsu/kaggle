# -*- coding: utf-8 -*-
"""house_price.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_WKdCIXASR1Zg99ClRoAN8Ly8vDdZs2S
"""

import numpy as np 
import pandas as pd 
import matplotlib
import matplotlib.pyplot as plt 
import seaborn as sns 
import warnings 
from sklearn.preprocessing import LabelEncoder #數據清洗 #對非連續數值或文本編碼
from sklearn.feature_selection import RFECV #交叉驗證預防過度擬合
from IPython.display import display
from IPython.display import display_html# for display dataframe #顯示繪製圖片 
from sklearn.ensemble import RandomForestClassifier 
from sklearn.model_selection import cross_val_score, StratifiedKFold, learning_curve, train_test_split
import json
import os
from sklearn.model_selection import GridSearchCV
from numpy import dtype

api_token={"username":"","key":""}

if not os.path.exists("/root/.kaggle"): 
  os.makedirs("/root/.kaggle")

with open('/root/.kaggle/kaggle.json', 'w') as file:
  json.dump(api_token,file)

!chmod 600 /root/.kaggle/kaggle.json

if not os.path.exists("/kaggle"):
  os.makedirs("/kaggle")
os.chdir('/kaggle') #修改當前

!kaggle competitions download -c house-prices-advanced-regression-techniques
!unzip house-prices-advanced-regression-techniques

df_test = pd.read_csv('test.csv')
df_train = pd.read_csv('train.csv')
df_submission = pd.read_csv('sample_submission.csv')
df_data = df_train.append(df_test)

df_data.info()

# print(df_train)
print(df_test)
# print(df_data["LotFrontage"])
# print(df_submission)
if df_data.Id.nunique() == df_train.shape[0] : #shape[0] 查看矩陣行數 行數＝不同値的數，相當於每個都不一樣
  print('Id 沒有重複.')
else: 
  print('有重複，找出重複值') #測試過並沒有重複 有有bug


# L=[] 找出重複值
# L2=[]
# for x in df_data["Id"]:
#     L.append(x)

# for y in L:
#   if L.count(y) > 1:
#      L2.append(y)
# print(L2)

  #   L2 = []
  # for e in set(L):
  #   if L.count(e) > 1:
  #     L2.append(e)
# print(L2)
print(df_submission)
print(df_test)

# from collections import Counter
# if dtype(df_train['GarageQual'].head()) == dtype(df_train['GarageQual'].head()):
#   print(Counter(df_train['GarageQual']))
#   # print(df_train_Cdrop[x])
lent=[]
com=[]
com6=[]
com10=[]
com4=[]
com5=[]
com80=[]
com20=[]
com300=[]
lent1=[]
com1=[]

df_train_Cdrop = df_train.drop(["Id","PoolQC","Alley","FireplaceQu","Fence","MiscFeature"],axis=1)
df_test_Cdrop = df_test.drop(["Id","PoolQC","Alley","FireplaceQu","Fence","MiscFeature"],axis=1)

df_train_Cdrop1= df_train_Cdrop.drop(["SalePrice"],axis=1)
df_train_Cdrop2= df_train_Cdrop.drop(["SalePrice"],axis=1)
df_test_Cdrop1 = df_test_Cdrop
df_test_Cdrop2 = df_test_Cdrop



from sklearn.preprocessing import LabelEncoder

for x in df_train_Cdrop1 : 
  if dtype(df_train_Cdrop1[x].head()) == dtype(df_train_Cdrop1['GarageQual'].head()):
    labelencoder = LabelEncoder()
    df_train_Cdrop1[x] = labelencoder.fit_transform(df_train_Cdrop1[x])
    # print(df_train_Cdrop1[x] )
    com20.append(df_train_Cdrop1[x]) ##
    # del df_train_Cdrop1[x]
# df_train_Cdrop2.info()

print(len(com20))
# for x in com20:
#   print(x)

for x in df_test_Cdrop1 : 
  if dtype(df_test_Cdrop1[x].head()) == dtype(df_test_Cdrop1['GarageQual'].head()):
    labelencoder = LabelEncoder()
    df_test_Cdrop1[x] = labelencoder.fit_transform(df_test_Cdrop1[x])
    # print(df_test_Cdrop1[x] )
    y=df_test_Cdrop1[x]
    # com20.append(df_train_Cdrop1[x])
    # com20.append(y.to_frame()) ##

# def PdCutobject (x):
#   if dtype(df_train[x].head()) == dtype(df_train['GarageQual'].head()):
#     # print(1)
#     print(df_train_Cdrop[x])
#     print(df_train_Cdrop[x],Name)
#     # print(df_train_Cdrop[x])

# for x in df_train_Cdrop:
#   PdCutobject(x)

com20=pd.concat(com20,axis=1)

# type(com20)
# print(m)
# df_train_Cdrop.info #資料清洗資料清洗 只調用原始數值

df_train_Cdrop2.info
# df_train_Cdrop1.info

##drop多到困難補值的特徵

com201=[]


def PdCut (x):
  #資料型別多樣 int64 float64 object category ，pd.cut可以在資料缺值不多的時候簡易分類當特徵使用，但限定只有數值可以使用
  #所以先所以先drop掉缺值太多的欄位，再用if篩選出可以用的資料屬性
  if dtype(df_train_Cdrop2[x].head()) == dtype(df_train_Cdrop2['MSSubClass'].head()) or dtype(df_train_Cdrop2[x].head()) == dtype(df_train_Cdrop2["LotFrontage"].head()):
    df_train_Cdrop2[x+"_4"] = pd.cut(df_train_Cdrop2[x], 4) 
    df_train_Cdrop2[x+"_5"] = pd.cut(df_train_Cdrop2[x], 5) 
    df_train_Cdrop2[x+"_6"] = pd.cut(df_train_Cdrop2[x], 6)
    df_train_Cdrop2[x+"_10"] = pd.cut(df_train_Cdrop2[x], 10)
    df_train_Cdrop2[x+"_20"] = pd.cut(df_train_Cdrop2[x], 20)
    df_train_Cdrop2[x+"_300"] = pd.cut(df_train_Cdrop2[x], 300)
    lent.append(x)
    # print(df_train_Cdrop[x+"_4"])
    label = LabelEncoder()
    df_train_Cdrop2[x+'_Code_4'] = label.fit_transform(df_train_Cdrop2[x+'_4'])
    df_train_Cdrop2[x+'_Code_5'] = label.fit_transform(df_train_Cdrop2[x+'_5'])
    df_train_Cdrop2[x+'_Code_6'] = label.fit_transform(df_train_Cdrop2[x+'_6'])
    df_train_Cdrop2[x+'_Code_10'] = label.fit_transform(df_train_Cdrop2[x+'_10'])
    df_train_Cdrop2[x+'_Code_20'] = label.fit_transform(df_train_Cdrop2[x+'_20'])
    df_train_Cdrop2[x+'_Code_300'] = label.fit_transform(df_train_Cdrop2[x+'_300'])
    com201.append(df_train_Cdrop2[x+'_Code_20'])
    com201.append(df_train_Cdrop2[x+'_Code_10'])#下面都可以不用，但就想試試跑個一萬年
    com201.append(df_train_Cdrop2[x+'_Code_6'])
    com201.append(df_train_Cdrop2[x+'_Code_5'])
    com201.append(df_train_Cdrop2[x+'_Code_4'])
    com201.append(df_train_Cdrop2[x+'_Code_300'])
    # com201.append(df_train_Cdrop2[x+'_Code_80'])
    com10.append(x+'_Code_10') 
    com6.append(x+'_Code_6')
    com5.append(x+'_Code_5')
    com4.append(x+'_Code_4')
    # com80.append(x+'_Code_80')
    com300.append(x+'_Code_300')

for y in df_train_Cdrop2:
  PdCut(y)
  # print(y)

print("特徵總數：",len(com201))
print("特徵名稱：",lent)
print(com201)


# df_train_Cdrop['FareBin_Code_4'] = label.fit_transform(df_train_Cdrop['FareBin_4'])





def PdCut1 (a):
  #資料型別多樣 int64 float64 object category ，pd.cut可以在資料缺值不多的時候簡易分類當特徵使用，但限定只有數值可以使用
  #所以先所以先drop掉缺值太多的欄位，再用if篩選出可以用的資料屬性
  if dtype(df_test_Cdrop1[a].head()) == dtype(df_test_Cdrop1['MSSubClass'].head()) or dtype(df_test_Cdrop1[a].head()) == dtype(df_test_Cdrop1["LotFrontage"].head()):
    df_test_Cdrop1[a+"_4"] = pd.cut(df_test_Cdrop1[a], 4) 
    df_test_Cdrop1[a+"_5"] = pd.cut(df_test_Cdrop1[a], 5) 
    df_test_Cdrop1[a+"_6"] = pd.cut(df_test_Cdrop1[a], 6)
    df_test_Cdrop1[a+"_10"] = pd.cut(df_test_Cdrop1[a], 10)
    df_test_Cdrop1[a+"_20"] = pd.cut(df_test_Cdrop1[a], 20)
    df_test_Cdrop1[a+"_80"] = pd.cut(df_test_Cdrop1[a], 80)
    df_test_Cdrop1[a+"_300"] = pd.cut(df_test_Cdrop1[a], 300)
    lent1.append(a)
    # print(df_train_Cdrop[x+"_4"])
    label = LabelEncoder()
    df_test_Cdrop1[a+'_Code_4'] = label.fit_transform(df_test_Cdrop1[a+'_4']) #一直有擬合不夠的問題，考慮把值拆的更碎拆的更碎 20以上試試看以上試試看 反正資料量超大
    df_test_Cdrop1[a+'_Code_5'] = label.fit_transform(df_test_Cdrop1[a+'_5']) #因為只用了原資料為數字的資料
    df_test_Cdrop1[a+'_Code_6'] = label.fit_transform(df_test_Cdrop1[a+'_6']) #其實還有接近40種文字資料沒有使用到，加進去預測率可能會好更多
    df_test_Cdrop1[a+'_Code_10'] = label.fit_transform(df_test_Cdrop1[a+'_10'])
    df_test_Cdrop1[a+'_Code_20'] = label.fit_transform(df_test_Cdrop1[a+'_20'])
    df_test_Cdrop1[a+'_Code_80'] = label.fit_transform(df_test_Cdrop1[a+'_80'])
    df_test_Cdrop1[a+'_Code_300'] = label.fit_transform(df_test_Cdrop1[a+'_300'])
    com1.append(a+'_Code_300')


# print(df_train_Cdrop['MSSubClass_Code_300'])

for b in df_test_Cdrop1:
  PdCut1(b)

print(com201)
# com201.info

com201=pd.concat(com201,axis=1)
com20=pd.concat((com20,com201),axis=1)

# print("特徵總數：",len(lent1))
# print("特徵名稱：",lent1)
print(com20)
type(com20)

# com20.info
com202=[]
for x in com20:
  # print(com20[x])
  com202.append(x)

# for x in df_train:
#   print(df_train[x])

from pandas.core.arrays import boolean
#程式碼儲存格 <gjcWYWIa2WGD>

# X = df_train_Cdrop.drop(labels=['SalePrice'],axis=1)
# Y = df_train_Cdrop['SalePrice']
# print(X)
# X.columns
# print(X.columns)

# del com[-1]
# print(com)
# print(X)
# compare = com
# selector = RFECV(RandomForestClassifier(n_estimators=250,min_samples_split=20),cv=10,n_jobs=-1) #RFECV比較精確但花費時間大，有其他較粗糙但快的方法
# selector.fit(X[compare], Y)

# com20 = com20.drop(["SaleType","SaleCondition"],axis=1)
# SaleType  SaleCondition
# X=[]
# for x in df_train_Cdrop:
#   if dtype(df_train_Cdrop[x].head())!=bool:
#     X.append(df_train_Cdrop[x])
# X = pd.concat((df_train_Cdrop1,df_train_Cdrop2),axis=1)
# X= X.drop(["SaleType","SaleCondition"],axis=1)
X=com20

X.info
# .drop(labels=['SalePrice'],axis=1)
# pd.concat
# X=X.to_frame()

# .append(df_train_Cdrop[com])
Y = df_train['SalePrice']
print(X)
X.columns
print("-"*80)
print(X.columns)
print("-"*80)
# del com[-1]
# print(com)
print("-"*80)
print(X)

# for y in X:
#   if dtype(df_train_Cdrop1[y].head())==bool: #or  dtype(df_train_Cdrop2[y].head())== False:
#     print(df_train_Cdrop1[y])

www=[]
# for qqq in df_train_Cdrop1.drop(labels=['SalePrice'],axis=1):
  # www.append(qqq)



# compare = ['MSSubClass_Code_20', 'LotFrontage_Code_20', 'LotArea_Code_20', 'OverallQual_Code_20', 'OverallCond_Code_20', 'YearBuilt_Code_20', 'YearRemodAdd_Code_20', 'MasVnrArea_Code_20', 'BsmtFinSF1_Code_20', 'BsmtFinSF2_Code_20', 'BsmtUnfSF_Code_20', 'TotalBsmtSF_Code_20', '1stFlrSF_Code_20', '2ndFlrSF_Code_20', 'LowQualFinSF_Code_20', 'GrLivArea_Code_20', 'BsmtFullBath_Code_20', 'BsmtHalfBath_Code_20', 'FullBath_Code_20', 'HalfBath_Code_20', 'BedroomAbvGr_Code_20', 'KitchenAbvGr_Code_20', 'TotRmsAbvGrd_Code_20', 'Fireplaces_Code_20', 'GarageYrBlt_Code_20', 'GarageCars_Code_20', 'GarageArea_Code_20', 'WoodDeckSF_Code_20', 'OpenPorchSF_Code_20', 'EnclosedPorch_Code_20', '3SsnPorch_Code_20', 'ScreenPorch_Code_20', 'PoolArea_Code_20', 'MiscVal_Code_20', 'MoSold_Code_20', 'YrSold_Code_20']#com#.append(www)
compare =com202

# compare = com4
# compare = com6
#
# compare=['MSSubClass_Code_20', 'MSSubClass_Code_10', 'MSSubClass_Code_6', 'MSSubClass_Code_5', 'MSSubClass_Code_4', 'LotFrontage_Code_20', 'LotFrontage_Code_10', 'LotFrontage_Code_6', 'LotFrontage_Code_5', 'LotFrontage_Code_4', 'LotArea_Code_20', 'LotArea_Code_10', 'LotArea_Code_6', 'LotArea_Code_5', 'LotArea_Code_4', 'OverallQual_Code_20', 'OverallQual_Code_10', 'OverallQual_Code_6', 'OverallQual_Code_5', 'OverallQual_Code_4', 'OverallCond_Code_20', 'OverallCond_Code_10', 'OverallCond_Code_6', 'OverallCond_Code_5', 'OverallCond_Code_4', 'YearBuilt_Code_20', 'YearBuilt_Code_10', 'YearBuilt_Code_6', 'YearBuilt_Code_5', 'YearBuilt_Code_4', 'YearRemodAdd_Code_20', 'YearRemodAdd_Code_10', 'YearRemodAdd_Code_6', 'YearRemodAdd_Code_5', 'YearRemodAdd_Code_4', 'MasVnrArea_Code_20', 'MasVnrArea_Code_10', 'MasVnrArea_Code_6', 'MasVnrArea_Code_5', 'MasVnrArea_Code_4', 'BsmtFinSF1_Code_20', 'BsmtFinSF1_Code_10', 'BsmtFinSF1_Code_6', 'BsmtFinSF1_Code_5', 'BsmtFinSF1_Code_4', 'BsmtFinSF2_Code_20', 'BsmtFinSF2_Code_10', 'BsmtFinSF2_Code_6', 'BsmtFinSF2_Code_5', 'BsmtFinSF2_Code_4', 'BsmtUnfSF_Code_20', 'BsmtUnfSF_Code_10', 'BsmtUnfSF_Code_6', 'BsmtUnfSF_Code_5', 'BsmtUnfSF_Code_4', 'TotalBsmtSF_Code_20', 'TotalBsmtSF_Code_10', 'TotalBsmtSF_Code_6', 'TotalBsmtSF_Code_5', 'TotalBsmtSF_Code_4', '1stFlrSF_Code_20', '1stFlrSF_Code_10', '1stFlrSF_Code_6', '1stFlrSF_Code_5', '1stFlrSF_Code_4', '2ndFlrSF_Code_20', '2ndFlrSF_Code_10', '2ndFlrSF_Code_6', '2ndFlrSF_Code_5', '2ndFlrSF_Code_4', 'LowQualFinSF_Code_20', 'LowQualFinSF_Code_10', 'LowQualFinSF_Code_6', 'LowQualFinSF_Code_5', 'LowQualFinSF_Code_4', 'GrLivArea_Code_20', 'GrLivArea_Code_10', 'GrLivArea_Code_6', 'GrLivArea_Code_5', 'GrLivArea_Code_4', 'BsmtFullBath_Code_20', 'BsmtFullBath_Code_10', 'BsmtFullBath_Code_6', 'BsmtFullBath_Code_5', 'BsmtFullBath_Code_4', 'BsmtHalfBath_Code_20', 'BsmtHalfBath_Code_10', 'BsmtHalfBath_Code_6', 'BsmtHalfBath_Code_5', 'BsmtHalfBath_Code_4', 'FullBath_Code_20', 'FullBath_Code_10', 'FullBath_Code_6', 'FullBath_Code_5', 'FullBath_Code_4', 'HalfBath_Code_20', 'HalfBath_Code_10', 'HalfBath_Code_6', 'HalfBath_Code_5', 'HalfBath_Code_4', 'BedroomAbvGr_Code_20', 'BedroomAbvGr_Code_10', 'BedroomAbvGr_Code_6', 'BedroomAbvGr_Code_5', 'BedroomAbvGr_Code_4', 'KitchenAbvGr_Code_20', 'KitchenAbvGr_Code_10', 'KitchenAbvGr_Code_6', 'KitchenAbvGr_Code_5', 'KitchenAbvGr_Code_4', 'TotRmsAbvGrd_Code_20', 'TotRmsAbvGrd_Code_10', 'TotRmsAbvGrd_Code_6', 'TotRmsAbvGrd_Code_5', 'TotRmsAbvGrd_Code_4', 'Fireplaces_Code_20', 'Fireplaces_Code_10', 'Fireplaces_Code_6', 'Fireplaces_Code_5', 'Fireplaces_Code_4', 'GarageYrBlt_Code_20', 'GarageYrBlt_Code_10', 'GarageYrBlt_Code_6', 'GarageYrBlt_Code_5', 'GarageYrBlt_Code_4', 'GarageCars_Code_20', 'GarageCars_Code_10', 'GarageCars_Code_6', 'GarageCars_Code_5', 'GarageCars_Code_4', 'GarageArea_Code_20', 'GarageArea_Code_10', 'GarageArea_Code_6', 'GarageArea_Code_5', 'GarageArea_Code_4', 'WoodDeckSF_Code_20', 'WoodDeckSF_Code_10', 'WoodDeckSF_Code_6', 'WoodDeckSF_Code_5', 'WoodDeckSF_Code_4', 'OpenPorchSF_Code_20', 'OpenPorchSF_Code_10', 'OpenPorchSF_Code_6', 'OpenPorchSF_Code_5', 'OpenPorchSF_Code_4', 'EnclosedPorch_Code_20', 'EnclosedPorch_Code_10', 'EnclosedPorch_Code_6', 'EnclosedPorch_Code_5', 'EnclosedPorch_Code_4', '3SsnPorch_Code_20', '3SsnPorch_Code_10', '3SsnPorch_Code_6', '3SsnPorch_Code_5', '3SsnPorch_Code_4', 'ScreenPorch_Code_20', 'ScreenPorch_Code_10', 'ScreenPorch_Code_6', 'ScreenPorch_Code_5', 'ScreenPorch_Code_4', 'PoolArea_Code_20', 'PoolArea_Code_10', 'PoolArea_Code_6', 'PoolArea_Code_5', 'PoolArea_Code_4', 'MiscVal_Code_20', 'MiscVal_Code_10', 'MiscVal_Code_6', 'MiscVal_Code_5', 'MiscVal_Code_4', 'MoSold_Code_20', 'MoSold_Code_10', 'MoSold_Code_6', 'MoSold_Code_5', 'MoSold_Code_4', 'YrSold_Code_20', 'YrSold_Code_10', 'YrSold_Code_6', 'YrSold_Code_5', 'YrSold_Code_4', 'SalePrice_Code_20', 'SalePrice_Code_10', 'SalePrice_Code_6', 'SalePrice_Code_5', 'SalePrice_Code_4']
print(compare)

selector = RFECV(RandomForestClassifier(random_state=20,n_estimators=400,min_samples_split=5,oob_score=True,min_samples_leaf=20,n_jobs=-1,max_leaf_nodes=500),cv=10,n_jobs=-1,min_features_to_select=19) #RFECV比較精確但花費時間大，有其他較粗糙但快的方法
selector.fit(X[compare],Y)





print("-"*80)
print(selector.n_features_)#覺得有效的參數的數量
print("-"*80)
print(selector.support_) #返回選中的參數 True False
print("-"*80)
print(selector.ranking_) #重要度排名
print("-"*80)
print(selector.cv_results_) #從最有影響力的特徵開始加入，計算使用多少個特徵對應得到的準確率。
print("-"*80)


# for m in  selector.support_:
#   # if selector.support_ == True:
#   rank.append(m)
# print(rank)

xxx= selector.support_

# xxx=[False, True, False, False, False, True, True, False, False, False, True, False, True, False, False, True, False, False, False, False, False, False, True, False, True, False, True, True, True, False, False, False, False]
j=0
yyy=[]

for i in  xxx:
  j=j+1
  if i == True:
    yyy.append(j)
print(yyy)

zzz=[]
aaa=[]

j=0
for k in compare:
  zzz.append(k)
  j=j+1
  if j==2:
    aaa.append(k)
# print(zzz)
for l in yyy:
  # print(zzz[l])
  aaa.append(zzz[l-1])


print(aaa)
print(len(aaa))
##調整重點 1.確認模擬的資料有哪些 2.判斷出最重要的資料是哪些然後拿來用 3.簡化取出的取出的for迴圈量迴圈量

# delete,
from sklearn.metrics import roc_auc_score
GridSearchCV,

# b20 = ['MSSubClass_Code_20', 'LotFrontage_Code_20', 'LotArea_Code_20', 'OverallQual_Code_20', 'OverallCond_Code_20', 'YearBuilt_Code_20', 'YearRemodAdd_Code_20', 'MasVnrArea_Code_20', 'BsmtFinSF1_Code_20', 'BsmtFinSF2_Code_20', 'BsmtUnfSF_Code_20', 'TotalBsmtSF_Code_20', '1stFlrSF_Code_20', '2ndFlrSF_Code_20', 'LowQualFinSF_Code_20', 'GrLivArea_Code_20', 'BsmtFullBath_Code_20', 'BsmtHalfBath_Code_20', 'FullBath_Code_20', 'HalfBath_Code_20', 'BedroomAbvGr_Code_20', 'KitchenAbvGr_Code_20', 'TotRmsAbvGrd_Code_20', 'Fireplaces_Code_20', 'GarageYrBlt_Code_20', 'GarageCars_Code_20', 'GarageArea_Code_20', 'WoodDeckSF_Code_20', 'OpenPorchSF_Code_20', 'EnclosedPorch_Code_20', '3SsnPorch_Code_20', 'ScreenPorch_Code_20', 'PoolArea_Code_20', 'MiscVal_Code_20', 'MoSold_Code_20', 'YrSold_Code_20']
b20=aaa #要用篩選後的話，b20=aaa
print(b20)
t1={"n_estimators":range(1,500,100)}
# del b20[-1]

g=GridSearchCV(estimator=RandomForestClassifier(random_state=10,n_estimators=401,min_samples_split=5,oob_score=True,min_samples_leaf=20,n_jobs=-1,max_leaf_nodes=500),param_grid=t1)
g.fit(X[b20], Y)
print(g.cv_results_)
print(g.best_params_)
print("best accuracy:%f" % g.best_score_)

# print(len([8, 9, 10, 13, 15, 16, 17, 18, 20, 25, 26, 31, 35, 36, 37, 38, 39, 41, 55, 56, 61, 62, 63, 64, 65, 66, 67, 68, 69, 73, 74, 79, 85, 86, 91, 97, 98, 103, 104, 109, 110, 115, 116, 121, 122, 127, 133, 134, 135, 139, 152, 153, 154, 155, 156, 164, 165, 169, 176, 177, 181, 188, 193, 194, 195, 200, 201, 205, 206, 211, 212, 217, 223, 225, 229, 230, 235, 247]))

b20_Model = RandomForestClassifier(random_state=10,n_estimators=401,min_samples_split=5,oob_score=True,min_samples_leaf=20,n_jobs=-1,max_leaf_nodes=500)
b20_Model.fit(X[b20], Y)

print('b20 oob score : %.5f' %(b20_Model.oob_score_)) 
print(len(b20))
print(b20)

# X = df_train_Cdrop.drop(labels=['SalePrice'],axis=1)
# Y = df_train_Cdrop['SalePrice']
# df_test_Cdrop.columns
X_Submit = df_test_Cdrop
# print(X_Submit)
# X_Submit.columns

print(X_Submit)

b20_pred = b20_Model.predict(X_Submit[b20])
submit = pd.DataFrame({"Id": df_test['Id'], "SalePrice":b20_pred.astype(int)})

print(submit)

note=[]

submit.to_csv("submit.csv",index=None)
!kaggle competitions submit -c house-prices-advanced-regression-techniques -f submit.csv -m "cv(min feature=19)random_state=10,n_estimators=401,min_samples_split=5,oob_score=True,min_samples_leaf=20,n_jobs=-1,max_leaf_nodes=500"
                                   
# from google.colab import drive
# drive.mount('/content/drive')

# submit.to_csv('/content/drive/MyDrive/price_b22.csv',index=None)